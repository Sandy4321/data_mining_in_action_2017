{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Считываем данные, делим товары на id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "with open('train.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        tokens = line.strip().split(' ')\n",
    "        data.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['942', '691']\n"
     ]
    }
   ],
   "source": [
    "print data[42]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаём датасет (убираем один товар, записываем остаток, добавляем удалённый объект как target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "\n",
    "for order in data:\n",
    "    for i in xrange(len(order)):\n",
    "        dataset.append({\n",
    "            'data': order[:i] + order[(i + 1):],\n",
    "            'target': order[i]\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "240165"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': ['580', '30', '804', '654', '25'], 'target': '743'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[42]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найдём универсум - множество всех id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "universe = set()\n",
    "for order in data:\n",
    "    universe.update(order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "992"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(universe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "universe_list = list(universe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Код для сэмплирования негативных примеров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_sample(dataset_obj, neg_samples_count=5):\n",
    "    samples = list(np.random.choice(universe_list, neg_samples_count))\n",
    "    samples.append(dataset_obj['target'])\n",
    "    res = dict(dataset_obj)\n",
    "    res['canidates'] = samples\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделим обучающую выборку на train и test (правильно ли так разбивать?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = dataset[:200000], dataset[200000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем два dataset-a для обучения и тестирования. В тесте сделаем побольше негативных примеров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_for_learning = map(lambda o: neg_sample(o, 5), train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_for_testing = map(lambda o: neg_sample(o, 50), test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На сколько такой test корректен для оценки качестве рекомендаций?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'canidates': ['233', '123', '934', '404', '898', '743'],\n",
       " 'data': ['580', '30', '804', '654', '25'],\n",
       " 'target': '743'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_for_learning[42]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "baseline модель в нашем случае это vowpall wabbit, поэтому создадим датасеты в нужном формате"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset_train', 'w') as f:\n",
    "    for obj in dataset_for_learning:\n",
    "        for candidate in obj['canidates']:\n",
    "            f.write('{} |Data {} |Candidate {}\\n'.format(\n",
    "                int(candidate == obj['target']) * 2 - 1, \n",
    "                ' '.join(obj['data']),\n",
    "                candidate\n",
    "            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset_test', 'w') as f:\n",
    "    for obj in dataset_for_testing:\n",
    "        for candidate in obj['canidates']:\n",
    "            f.write('{} |Data {} |Candidate {}\\n'.format(\n",
    "                int(candidate == obj['target']) * 2 - 1, \n",
    "                ' '.join(obj['data']),\n",
    "                candidate\n",
    "            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поэкспериментируем с разными параметрами при обучении"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!rm dataset_train.cache\n",
    "!vw -d dataset_train  -c --passes 10 -f vw.model --binary --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!vw -i vw.model -t dataset_test -p dataset_test.out --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нужно считать предсказания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "recommendations = []\n",
    "with open('dataset_test.out', 'r') as f:\n",
    "    for obj in dataset_for_testing:\n",
    "        rec = []\n",
    "        for candidate in obj['canidates']:\n",
    "            rec.append((candidate, float(f.readline()), candidate == obj['target'])) \n",
    "        recommendations.append(sorted(rec, key=lambda x: -x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Код для оценки качества рекомендаций. Пусть пока будет recall@k для разных k.\n",
    "\n",
    "Какие ещё метрики кроме recall@k вы бы смотрели в данной задаче? (имеется в виду не конкретно в нашем бизнес кейса, а вообще в подобной рекомендации)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_quality(recommendations):\n",
    "    recalls_at_k = np.zeros(100)\n",
    "    count = 0\n",
    "    for rec in recommendations:\n",
    "        hitted = False\n",
    "        count += 1\n",
    "        for i, (candidate, prediction, true_relevance) in enumerate(rec):\n",
    "            hitted |= true_relevance\n",
    "            recalls_at_k[i] += hitted\n",
    "    for pos, val in enumerate(recalls_at_k * 1. / count):\n",
    "        print pos + 1, round(val, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.063\n",
      "2 0.116\n",
      "3 0.164\n",
      "4 0.207\n",
      "5 0.247\n",
      "6 0.284\n",
      "7 0.32\n",
      "8 0.353\n",
      "9 0.384\n",
      "10 0.413\n",
      "11 0.441\n",
      "12 0.466\n",
      "13 0.493\n",
      "14 0.516\n",
      "15 0.54\n",
      "16 0.563\n",
      "17 0.586\n",
      "18 0.608\n",
      "19 0.629\n",
      "20 0.649\n",
      "21 0.668\n",
      "22 0.686\n",
      "23 0.705\n",
      "24 0.723\n",
      "25 0.74\n",
      "26 0.757\n",
      "27 0.772\n",
      "28 0.788\n",
      "29 0.804\n",
      "30 0.818\n",
      "31 0.832\n",
      "32 0.844\n",
      "33 0.857\n",
      "34 0.87\n",
      "35 0.882\n",
      "36 0.893\n",
      "37 0.904\n",
      "38 0.915\n",
      "39 0.927\n",
      "40 0.937\n",
      "41 0.947\n",
      "42 0.956\n",
      "43 0.965\n",
      "44 0.974\n",
      "45 0.98\n",
      "46 0.986\n",
      "47 0.991\n",
      "48 0.994\n",
      "49 0.997\n",
      "50 0.999\n",
      "51 1.0\n",
      "52 0.0\n",
      "53 0.0\n",
      "54 0.0\n",
      "55 0.0\n",
      "56 0.0\n",
      "57 0.0\n",
      "58 0.0\n",
      "59 0.0\n",
      "60 0.0\n",
      "61 0.0\n",
      "62 0.0\n",
      "63 0.0\n",
      "64 0.0\n",
      "65 0.0\n",
      "66 0.0\n",
      "67 0.0\n",
      "68 0.0\n",
      "69 0.0\n",
      "70 0.0\n",
      "71 0.0\n",
      "72 0.0\n",
      "73 0.0\n",
      "74 0.0\n",
      "75 0.0\n",
      "76 0.0\n",
      "77 0.0\n",
      "78 0.0\n",
      "79 0.0\n",
      "80 0.0\n",
      "81 0.0\n",
      "82 0.0\n",
      "83 0.0\n",
      "84 0.0\n",
      "85 0.0\n",
      "86 0.0\n",
      "87 0.0\n",
      "88 0.0\n",
      "89 0.0\n",
      "90 0.0\n",
      "91 0.0\n",
      "92 0.0\n",
      "93 0.0\n",
      "94 0.0\n",
      "95 0.0\n",
      "96 0.0\n",
      "97 0.0\n",
      "98 0.0\n",
      "99 0.0\n",
      "100 0.0\n"
     ]
    }
   ],
   "source": [
    "estimate_quality(recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm dataset_train.cache\n",
    "!vw -d dataset_train  -c --passes 2 -f vw.model -q DC --quiet --binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!vw -i vw.model -t dataset_test -p dataset_test.out --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations = []\n",
    "with open('dataset_test.out', 'r') as f:\n",
    "    for obj in dataset_for_testing:\n",
    "        rec = []\n",
    "        for candidate in obj['canidates']:\n",
    "            rec.append((candidate, float(f.readline()), candidate == obj['target'])) \n",
    "        recommendations.append(sorted(rec, key=lambda x: -x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.158\n",
      "2 0.231\n",
      "3 0.282\n",
      "4 0.325\n",
      "5 0.363\n",
      "6 0.395\n",
      "7 0.425\n",
      "8 0.452\n",
      "9 0.478\n",
      "10 0.504\n",
      "11 0.527\n",
      "12 0.549\n",
      "13 0.571\n",
      "14 0.591\n",
      "15 0.612\n",
      "16 0.63\n",
      "17 0.647\n",
      "18 0.664\n",
      "19 0.679\n",
      "20 0.695\n",
      "21 0.71\n",
      "22 0.723\n",
      "23 0.738\n",
      "24 0.751\n",
      "25 0.763\n",
      "26 0.774\n",
      "27 0.784\n",
      "28 0.793\n",
      "29 0.802\n",
      "30 0.809\n",
      "31 0.816\n",
      "32 0.823\n",
      "33 0.829\n",
      "34 0.835\n",
      "35 0.84\n",
      "36 0.843\n",
      "37 0.848\n",
      "38 0.851\n",
      "39 0.854\n",
      "40 0.856\n",
      "41 0.858\n",
      "42 0.859\n",
      "43 0.86\n",
      "44 0.861\n",
      "45 0.862\n",
      "46 0.863\n",
      "47 0.863\n",
      "48 0.863\n",
      "49 0.864\n",
      "50 0.864\n",
      "51 1.0\n",
      "52 0.0\n",
      "53 0.0\n",
      "54 0.0\n",
      "55 0.0\n",
      "56 0.0\n",
      "57 0.0\n",
      "58 0.0\n",
      "59 0.0\n",
      "60 0.0\n",
      "61 0.0\n",
      "62 0.0\n",
      "63 0.0\n",
      "64 0.0\n",
      "65 0.0\n",
      "66 0.0\n",
      "67 0.0\n",
      "68 0.0\n",
      "69 0.0\n",
      "70 0.0\n",
      "71 0.0\n",
      "72 0.0\n",
      "73 0.0\n",
      "74 0.0\n",
      "75 0.0\n",
      "76 0.0\n",
      "77 0.0\n",
      "78 0.0\n",
      "79 0.0\n",
      "80 0.0\n",
      "81 0.0\n",
      "82 0.0\n",
      "83 0.0\n",
      "84 0.0\n",
      "85 0.0\n",
      "86 0.0\n",
      "87 0.0\n",
      "88 0.0\n",
      "89 0.0\n",
      "90 0.0\n",
      "91 0.0\n",
      "92 0.0\n",
      "93 0.0\n",
      "94 0.0\n",
      "95 0.0\n",
      "96 0.0\n",
      "97 0.0\n",
      "98 0.0\n",
      "99 0.0\n",
      "100 0.0\n"
     ]
    }
   ],
   "source": [
    "estimate_quality(recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm dataset_train.cache\n",
    "!vw -d dataset_train  -c --passes 10 -f vw.model -q DC --quiet --binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "!vw -i vw.model -t dataset_test -p dataset_test.out --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations = []\n",
    "with open('dataset_test.out', 'r') as f:\n",
    "    for obj in dataset_for_testing:\n",
    "        rec = []\n",
    "        for candidate in obj['canidates']:\n",
    "            rec.append((candidate, float(f.readline()), candidate == obj['target'])) \n",
    "        recommendations.append(sorted(rec, key=lambda x: -x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.144\n",
      "2 0.216\n",
      "3 0.265\n",
      "4 0.308\n",
      "5 0.345\n",
      "6 0.378\n",
      "7 0.408\n",
      "8 0.436\n",
      "9 0.462\n",
      "10 0.487\n",
      "11 0.511\n",
      "12 0.532\n",
      "13 0.554\n",
      "14 0.573\n",
      "15 0.593\n",
      "16 0.612\n",
      "17 0.63\n",
      "18 0.646\n",
      "19 0.662\n",
      "20 0.676\n",
      "21 0.689\n",
      "22 0.702\n",
      "23 0.713\n",
      "24 0.723\n",
      "25 0.732\n",
      "26 0.741\n",
      "27 0.749\n",
      "28 0.756\n",
      "29 0.762\n",
      "30 0.768\n",
      "31 0.773\n",
      "32 0.778\n",
      "33 0.781\n",
      "34 0.785\n",
      "35 0.787\n",
      "36 0.789\n",
      "37 0.791\n",
      "38 0.793\n",
      "39 0.794\n",
      "40 0.795\n",
      "41 0.796\n",
      "42 0.796\n",
      "43 0.797\n",
      "44 0.798\n",
      "45 0.798\n",
      "46 0.798\n",
      "47 0.799\n",
      "48 0.799\n",
      "49 0.8\n",
      "50 0.8\n",
      "51 1.0\n",
      "52 0.0\n",
      "53 0.0\n",
      "54 0.0\n",
      "55 0.0\n",
      "56 0.0\n",
      "57 0.0\n",
      "58 0.0\n",
      "59 0.0\n",
      "60 0.0\n",
      "61 0.0\n",
      "62 0.0\n",
      "63 0.0\n",
      "64 0.0\n",
      "65 0.0\n",
      "66 0.0\n",
      "67 0.0\n",
      "68 0.0\n",
      "69 0.0\n",
      "70 0.0\n",
      "71 0.0\n",
      "72 0.0\n",
      "73 0.0\n",
      "74 0.0\n",
      "75 0.0\n",
      "76 0.0\n",
      "77 0.0\n",
      "78 0.0\n",
      "79 0.0\n",
      "80 0.0\n",
      "81 0.0\n",
      "82 0.0\n",
      "83 0.0\n",
      "84 0.0\n",
      "85 0.0\n",
      "86 0.0\n",
      "87 0.0\n",
      "88 0.0\n",
      "89 0.0\n",
      "90 0.0\n",
      "91 0.0\n",
      "92 0.0\n",
      "93 0.0\n",
      "94 0.0\n",
      "95 0.0\n",
      "96 0.0\n",
      "97 0.0\n",
      "98 0.0\n",
      "99 0.0\n",
      "100 0.0\n"
     ]
    }
   ],
   "source": [
    "estimate_quality(recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В нашей задаче какую-бы модель вы бы выбрали?\n",
    "\n",
    "Стоит ли верить полученным оценкам качества?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание\n",
    "\n",
    "#### Нужно поисследовать эти данные. Обучить рекомендательную систему и сделать предсказания для test.txt.\n",
    "#### Предсказания должны быть в таком формате:\n",
    "id1, id2, id3, ... (не более 20 на каждый чек, каждая запись в отдельной строке)\n",
    "\n",
    "например,\n",
    "\n",
    "1, 2, 3, 5\n",
    "\n",
    "1, 3, 4, 6, 7, 8\n",
    "\n",
    "2, 3, 4, 5\n",
    "\n",
    "...\n",
    "\n",
    "\n",
    "#### Форма для сдачи откроется позже\n",
    "\n",
    "#### Метрика качества не оглашается. Вы должны сами выбрать её для себя. Можете обсуждать в чатике) Учитывайте как мы решили модифицировать наш первый эксперимент, когда будете выбирать метрику.\n",
    "\n",
    "#### Если в ходе обсуждения в чате будет дан правильный или около правильный ответ про метрику, то финальная метрика качества будет оглашена."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вопросы и идеи для улучшения качества:\n",
    "    1. Можно поиграться с параметрами Vowpall Wabbit для улучшения качества\n",
    "    2. Возможно, нужно исправить схему оценки качества (по-другому делить на test-train, по-другому оценивать качество на test)\n",
    "    3. Обучать можно не только линейные модели, но и градиентный бустинг. Хотя для этого нужно придумать хорошее численное описание объектов.\n",
    "    4. Можно использовать алгоритмы кластеризации и разложения матриц для получения короткого описания чека\n",
    "    5. Можно подсчитать разные статистики по парам товаров (например, сколько раз встретились вместе в чеке) и потом использовать как признаки (например, среднее число раз, которое каждый товар из чека встречался вместе с кандидатом).\n",
    "    6. Подумайте как правильно считать эти статистики (не очень хорошо считать их по train-у так как это подглядывание в target фактически).\n",
    "    7. Возможно обучаться нужно не на просто на случайных негативных примерах (а может и на них))), или применять нужно не ко всему универсуму, а к какому-то его подмножеству (а может и нет))). Подумайте над этим."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
